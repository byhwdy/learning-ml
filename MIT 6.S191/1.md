00:02
good afternoon everyone thank you all
00:05
for joining us my name is Alexandra
00:07
Meany and one of the course organizers
00:09
for six s-191 this is mi t--'s official
00:12
course on introduction to deep learning
00:14
and this is actually the third year that
00:16
we're offering this course and we've got
00:19
a really good one in store for you this
00:20
year with a lot of awesome updates so I
00:22
really hope that you enjoy it
00:24
so what is this course all about this is
00:28
a one-week intensive boot camp on
00:30
everything deep learning
00:31
you'll get up close and personal with
00:33
some of the foundations of the
00:35
algorithms driving this remarkable field
00:37
and you'll actually learn how to build
00:39
some intelligent algorithms capable of
00:42
solving incredibly complex problems so
00:46
over the past couple years deep learning
00:48
has revolutionized many aspects of
00:51
research and industry including things
00:53
like autonomous vehicles medicine and
00:56
healthcare reinforcement learning
00:58
generative modeling robotics and a whole
01:02
host of other applications like natural
01:05
language processing finance and security
01:07
but before we talk about that I think we
01:09
should start by taking a step back and
01:11
talking about something at the core of
01:13
this class which is intelligence what is
01:16
intelligence well I like to define
01:19
intelligence as the ability to process
01:21
information to inform future decisions
01:24
the field of artificial intelligence is
01:27
actually building algorithms artificial
01:29
algorithms to do exactly that
01:31
bit process information to inform future
01:34
predictions now machine learning is
01:37
simply a subset of artificial
01:39
intelligence or AI that actually focuses
01:41
on teaching an algorithm how to take
01:45
information and do this without
01:48
explicitly being told the sequence of
01:50
rules but instead learn the sequence of
01:52
patterns from the data itself deep
01:56
learning is simply a subset of machine
01:58
learning which takes this idea one step
02:00
further and actually tries to extract
02:02
these patterns automatically from raw
02:04
data without being needed without the
02:07
need to for the human to actually come
02:10
in and annotate these rules that the
02:13
system needs to learn
02:14
and that's what this class is all about
02:18
teaching algorithms how to learn a task
02:20
from raw data we want to provide you
02:23
with a solid foundation that so that you
02:25
can learn how these algorithms work
02:27
under the hood and with the practical
02:29
skills so that you can actually
02:30
implement these algorithms from scratch
02:32
using deep learning frameworks like
02:35
tensor flow which is the current most
02:37
popular deep learning framework that you
02:39
can code some of neural networks and
02:42
deep learning model and other deep
02:44
learning models we have an amazing set
02:48
of lectures lined up for you this week
02:49
including today which will kick off an
02:51
introduction on neural networks and
02:54
sequence based modeling which you'll
02:56
hear about in the second part of the
02:57
class tomorrow we'll cover some about
03:00
some stuff about computer vision and
03:03
deep generative modeling and the day
03:08
after that we'll talk even about
03:09
reinforcement learning and end on some
03:12
of the challenges and limitations of the
03:15
current deep learning approaches and and
03:17
kind of touch on how we can move forward
03:19
as a field past these challenges we'll
03:22
also spend the final two days hearing
03:24
from some guest lectures from top a AI
03:27
researchers these are bound to be
03:31
extremely interesting though we have
03:34
speakers from Nvidia IBM Google coming
03:37
to give talks so I highly recommend
03:39
attending these as well and finally the
03:42
class will conclude with some final
03:43
project presentations from students like
03:46
you and the audience will where you'll
03:48
present some final projects for this
03:50
class and then we'll end on an award
03:52
ceremony to celebrate so as you might
03:56
have seen or heard already this class is
03:58
offered for credit you can take this
04:00
class for grade and if you're taking
04:02
this class for grade you have two
04:03
options to fulfill your grade
04:04
requirement first option is that you can
04:07
actually do a project proposal where you
04:10
will present your project on the final
04:12
day of class that's what I was saying
04:13
before on Friday you can present your
04:15
project and this is just a three minute
04:17
presentation we'll be very strict on the
04:19
time here and we realized that one week
04:22
is a super short time to actually come
04:24
up with a deep learning project so we're
04:25
not going to actually be judging you on
04:27
the results
04:28
you create during this week instead what
04:30
we're looking for is the novelty of the
04:32
ideas and how well you can present it
04:34
given such a short amount of time in
04:36
three minutes and we kind of think it's
04:40
like an art to being able to present
04:41
something in just three minutes so we
04:44
kind of want to hold you to that tight
04:46
time schedule and kind of enforce it
04:48
very tightly just so that you're forced
04:50
to really think about what is the core
04:52
idea that you want to present to us on
04:54
Friday your projects your presentations
04:58
will be judged by a panel of judges and
05:01
will be awarding GPUs and some home
05:05
Google home AI assistants this year
05:07
we're offering three NVIDIA GPUs each
05:10
one worth over $1,000 some of you know
05:13
these GPUs are the backbone of doing
05:16
cutting-edge deep learning research and
05:18
it's really foundational or essential if
05:21
you want to be doing this kind of
05:22
research so we're really happy that we
05:24
can offer you these types this type of
05:26
hardware the second option if you don't
05:30
want to do the project presentation but
05:32
you still want to receive credit for
05:33
this class you can do the second option
05:35
which is a little more boring in my
05:37
opinion but you can write a one-page
05:40
review of a deep learning paper and this
05:43
will be doing the last day of class and
05:45
this is for people I don't want to do
05:47
the project presentation but you still
05:48
want to get credit for this class please
05:53
post to Piazza if you have questions
05:55
about the labs that we'll be doing today
05:57
or any of the future days if you have
05:59
questions about the course in general
06:00
there's course information on the
06:02
website enter deep learning com
06:05
along with announcements digital
06:07
recordings as well as slides for these
06:09
classes today's slides are already
06:11
released so you can find everything
06:13
online and of course if you have any
06:15
questions you can email us at intro to
06:17
deep learning - staff at MIT edu this
06:20
course has an incredible team that you
06:22
can reach out to in case you have any
06:23
questions or issues about anything so
06:26
please don't hesitate to reach out and
06:28
finally we want to give a huge thanks to
06:30
all of the sponsors that made this
06:32
course possible so now let's start with
06:36
the fun stuff and actually let's start
06:39
by asking ourselves a question
06:41
why do we even care about this class why
06:44
did you all come here today what is why
06:46
do we care about deep learning well
06:47
traditional machine learning algorithms
06:50
typically define sets of rules or
06:53
features that you want to extract from
06:57
the data usually these are hand
06:59
engineered features and they tend to be
07:00
extremely brittle in practice
07:03
now the key idea is a key insight of
07:05
deep learning is that let's not hand
07:08
engineer these features instead let's
07:10
learn them directly from raw data that
07:13
is can we learn in order to detect the
07:16
face we can first detect the edges in
07:19
the picture compose these edges together
07:21
to start detecting things like eyes
07:24
mouth and nose and then composing these
07:27
features together to detect higher-level
07:30
structures in the face and and this is
07:34
all performed in a hierarchical manner
07:35
so the question of deep learning is how
07:37
can we go from raw image pixels or raw
07:40
data in general to a more complex and
07:44
complex representation as the data flows
07:46
through the model and actually the
07:50
fundamental fundamental building blocks
07:52
of deep learning have existed for
07:54
decades and their underlying algorithms
07:57
have been studied for many years even
08:00
before that so why are we studying this
08:02
now well for one data has become so
08:06
prevalent in today's society we're
08:08
living in the age of big data where we
08:11
have more access to data than ever
08:12
before and these models are hungry for
08:14
data so we need to feed them with all
08:17
the data and a lot of this datasets that
08:19
we have available like computer vision
08:21
datasets natural language processing
08:22
datasets this raw amount of data was
08:25
just not available when these algorithms
08:27
were created second these algorithms
08:30
require or these albums are massively
08:33
parallel lies about their core at their
08:36
most fundamental building blocks that
08:37
you'll learn today they're massively
08:39
paralyzed Abul and this means that they
08:41
can benefit tremendously from very
08:43
specialized hardware such as GPUs and
08:46
again technology like these GPUs simply
08:50
did not exist in the decades that deep
08:52
learning or the foundations of deep
08:54
learning were devil
08:55
and finally due to open-source tool
08:58
boxes like tensorflow which will you
08:59
learn to use in this class building and
09:01
deploying these models has become more
09:04
streamlined than ever before it is
09:05
becoming increasingly and increasingly
09:07
easy to abstract away all of the details
09:11
and build a neural network and train a
09:13
neural network and then deploy that
09:14
neural network in practice to solve a
09:17
very complex problem in just tens of
09:19
lines of code you can solve you can
09:21
create a facial classifier that's
09:23
capable of recognizing very complex
09:26
faces from the environment so let's
09:30
start with the most fundamental building
09:32
block of deep learning and that's the
09:35
fundamental building block that makes up
09:36
a neural network and that is a neuron so
09:39
what is the neuron in deep learning we
09:41
call it a perceptron and how does it
09:44
work so the idea of a perceptron or a
09:48
single neuron is very simple let's start
09:51
by talking about and describing the
09:53
feed-forward information of information
09:56
through that model we define a set of
09:59
inputs x1 through XM which you can see
10:02
on the left hand side and each of these
10:05
inputs are actually multiplied by a
10:07
corresponding weight w1 through WM so
10:13
you can imagine if you have x1 you x w1
10:16
you have x2 you x w2 and so on you take
10:20
all of those multiplications and you add
10:22
them up so these come together in a
10:24
summation and then you pass this
10:26
weighted sum through a nonlinear
10:29
activation function to produce a final
10:31
output which we'll call Y so that's
10:36
really simple let's I actually left out
10:40
one detail in that previous slide so
10:41
I'll add it here now we also have this
10:44
other turn term this green term which is
10:46
a bias term which allows you to shift
10:48
your activation function left and right
10:51
and now on the right side you can kind
10:54
of see this diagram illustrated as a
10:56
mathematical formula as a single
10:59
equation we can actually rewrite this
11:01
now using linear algebra using vectors
11:03
dot products and matrices so let's do
11:06
that so now
11:09
is a vector of our inputs x1 through M
11:12
so instead of now a single number X
11:14
capital X is a vector of all of the
11:17
inputs capital W is a vector of all of
11:20
the weights 1 through m and we can
11:22
simply take their weighted sum by taking
11:24
the dot product between these two
11:26
vectors then we add our bias like I said
11:30
before or biased now is a single number
11:33
W not and applying that non linear term
11:36
so the nonlinear term transfers that
11:38
transforms that scalar input to another
11:41
scalar output Y so you might now be
11:47
wondering what is this thing that I've
11:49
been referring to as an activation
11:50
function I've mentioned it a couple
11:52
times I called it by a couple different
11:53
names first was a nonlinear function
11:55
then was an activation function what is
11:58
it
11:58
so one common example of a nonlinear
12:01
activation function is called the
12:02
sigmoid function and you can see one
12:04
here defined on the bottom right this is
12:08
a function that takes as input any real
12:10
number and outputs a new number between
12:13
0 and 1 so you can see it's essentially
12:16
collapsing your input between this range
12:18
of 0 and 1 this is just one example of
12:21
an activation function but there are
12:22
many many many activation functions used
12:24
in neural networks
12:25
here are some common ones and throughout
12:28
this presentation you'll see these
12:29
tensorflow code blocks on the bottom
12:31
like like this for example and I'll just
12:35
be using these as a as a way to kind of
12:37
bridge the gap between the theories that
12:39
you'll learn in this class with some of
12:41
the tensor flow that you'll be
12:42
practicing in the labs later today and
12:45
through the week so the sigmoid function
12:49
like I mentioned before which you can
12:50
see on the left-hand side is useful for
12:52
modeling probabilities because like I
12:54
said it collapses your your input to
12:58
between 0 & 1
12:59
since probabilities are modeled between
13:01
0 & 1 this is actually the perfect
13:04
activation function for the end of your
13:05
neural network if you want to predict
13:07
probability distributions at the end
13:08
another popular option is the r lu
13:11
function which you can see on the far
13:13
right-hand side this function is an
13:15
extremely simple one to compute it's
13:17
piecewise linear and it's very popular
13:20
because it's so easy to compute but
13:22
has this non-linearity at Z equals zero
13:25
so at Z less than 0 this function equals
13:29
0 and at Z greater than 0 it just equals
13:31
the input and because of this
13:33
non-linearity it's still able to capture
13:35
all of the great properties of
13:36
activation functions while still being
13:38
extremely simple to compute and now I
13:42
want to talk a little bit about why do
13:45
we use activation functions at all I
13:47
think a great part of this class is to
13:49
actually ask questions and not take
13:51
anything for granted so if I tell you we
13:53
need an activation function the first
13:55
thing that should come to your mind is
13:56
well why do we need that activation
13:57
function so activation functions the
14:01
purpose of activation functions is to
14:03
introduce nonlinearities into the
14:05
network this is extremely important in
14:08
deep learning or in machine learning in
14:09
general because in real life data is
14:12
almost always very nonlinear imagine I
14:15
told you to separate here the green from
14:18
the red points you might think that's
14:20
easy but then what if I told you you had
14:21
to only use a single line to do it
14:24
well now it's impossible that actually
14:27
makes the problem not only really hard
14:28
like I said it makes it impossible in
14:30
fact if you use linear activation
14:31
functions in a neural network no matter
14:34
how deep or wide your neural network is
14:37
no matter how many neurons it has this
14:39
is the best that I will be able to do
14:41
produce a linear decision boundary
14:43
between the red and the green points and
14:45
that's because it's using linear
14:46
activation functions when we introduce a
14:49
nonlinear activation function that
14:51
allows us to approximate arbitrarily
14:53
complex functions and draw arbitrarily
14:56
complex decision boundaries in this
14:58
feature space and that's exactly what
15:00
makes neural networks so powerful in
15:02
practice so let's understand this with a
15:05
simple example imagine I give you a
15:08
trains Network with weights W on the top
15:11
here so W 0 is 1 and let's say W 0 is 1
15:17
the W vector is 3 negative 2 so this is
15:20
a trained neural network and I want to
15:23
feed in a new input to this network well
15:26
how do we compute the output remember
15:28
from before it's the dot product we add
15:31
our bias and we compute a non-linearity
15:33
there's three steps
15:35
so let's take a look at what's going on
15:38
here what's inside of this nonlinear
15:40
function the input to the nonlinear
15:42
function well this is just a 2d line in
15:45
fact we can actually plot this 2d line
15:48
in what we call the feature space so on
15:51
the x axis you can see X 1 which is the
15:54
first input and on the y axis you can
15:57
see X 2 which is the second input this
15:59
neural network has two inputs we can
16:02
plot the line when it is equal to zero
16:04
and you can actually see it in the
16:06
feature space here if I give you a new
16:09
point a new input to this neural network
16:11
you can also plot this new point in the
16:13
same feature space so here's the point
16:16
negative 1 2 you can plot it like this
16:19
and actually you can compute the output
16:21
by plugging it into this equation that
16:24
we created before this line if we plug
16:26
it in we get 1 minus 3 minus 4 right
16:30
which equals minus 6 that's the input to
16:32
our activation function and then when we
16:34
feed it through our activation function
16:36
here I'm using sigmoid again for example
16:38
our final output is zero point zero zero
16:41
two ok what does that number mean let's
16:45
go back to this illustration of the
16:46
feature space again what this feature
16:49
space is doing is essentially dividing
16:50
the space into two hyperplanes remember
16:54
that the sigmoid function outputs values
16:57
between 0 and 1 and at z equals 0 when
17:01
the input to the sigmoid is 0 the output
17:04
of the sigmoid is 0.5 so essentially
17:06
you're splitting your space into two
17:08
planes one where Z is greater than zero
17:10
and one more Z is less than zero and one
17:13
where Y is greater than 0.5 and one
17:15
where Y is less than 0.5 the two are
17:17
synonymous but when we're dealing with
17:21
small dimensional input data like here
17:23
we're dealing with only two dimensions
17:24
we can make these beautiful plots and
17:27
these are very valuable and actually
17:29
visualizing the learning algorithm
17:31
visualizing how our output is relating
17:33
to our input we're gonna find very soon
17:35
that we can't really do this for all
17:38
problems because while here we're
17:41
dealing with only two inputs in
17:42
practical applications and deep neural
17:44
networks we're gonna be dealing with
17:46
hundreds thousands or even millions of
17:47
inputs to the network
17:48
at any given time and then drawing one
17:51
of these plots in thousand dimensional
17:53
space is going to become pretty tough so
17:58
now that we have an idea of the
17:59
perceptron a single neuron let's start
18:02
by building neural networks from the
18:04
ground up using one neuron and seeing
18:07
how this all comes together let's
18:10
revisit our diagram of the perceptron if
18:12
there's a few things that you remember
18:14
from this class I want to remember this
18:16
so there's three steps to computing the
18:17
output of a perceptron dot product add a
18:20
bias taking non-linearity three steps
18:24
let's simplify the diagram a little bit
18:26
I just got rid of the bias
18:27
I removed the weights just for
18:29
simplicity to keep things simple and
18:31
just note here that I'm writing Z as the
18:35
input to the to the activation function
18:38
so this is the weighted combination
18:41
essentially of your inputs Y is then
18:45
taking the activation function with
18:47
input Z so the final output like I said
18:52
Y is is on the right-hand side here and
18:55
it's the activation function applied to
18:58
this weighted sum if we want to define a
19:01
multi output neural network now all we
19:03
have to do is add another perceptron to
19:05
this picture now we have two outputs
19:08
each one is a normal perceptron like we
19:10
defined before no nothing extra and each
19:13
one is taking all the inputs from the
19:15
left-hand side computing this weighted
19:17
sum adding a bias and passing it through
19:21
an activation function let's keep going
19:24
now let's take a look at a single
19:26
layered neural network this is one where
19:28
we have a single hidden layer between
19:30
our inputs and our outputs we call it a
19:32
hidden layer because unlike the input
19:34
and the output which are strictly
19:36
observable or hidden layers learned so
19:39
we don't explicitly enforce any behavior
19:42
on the hidden layer and that's why we
19:43
call it hidden in that sense since we
19:46
now have a transformation from the
19:48
inputs to the hidden layer and hidden
19:50
layer to the outputs we're going to need
19:53
two weight matrices so we're going to
19:55
call it W one to go from input to hidden
19:58
layer and W two to go from hidden layer
20:02
to
20:02
output but again the story here's the
20:05
same dot product add a bias for each of
20:08
the neurons and then compute an
20:10
activation function let's zoom in now to
20:13
a single hidden hidden unit in this
20:15
hidden layer if we look at the single
20:17
unit take z2 for example it is just the
20:20
same perceptron that we saw before I'm
20:23
going to keep repeating myself
20:24
we took a dot product with the inputs we
20:26
applied a bias and then actually so
20:29
since it's Z we had not applied our
20:31
activation function yet so it's just a
20:33
dot product plus a bias so far if we
20:36
took it and took a look at a different
20:38
neuron let's say z3 or z4 the idea here
20:42
is gonna be the same but we're probably
20:43
going to end up with a different value
20:46
for Z 3 and C 4 just because the weights
20:48
leading from Z 3 to the inputs are going
20:51
to be different for each of those
20:52
neurons so this picture looks a little
20:55
bit messy so let's clean things up a
20:56
little bit more and just replace all of
20:58
these hidden layers all these lines
21:00
between the hidden layers with these
21:02
symbols these symbols denote fully
21:04
connected layers where each input to the
21:06
layer is connected to each output of the
21:09
layer another common name for these is
21:11
called dense layers and you can actually
21:15
write this in tensor flow using just
21:18
four lines of code so this neural
21:20
network which is a single layered multi
21:22
output neural network can be called by
21:25
instantiating your inputs feeding those
21:28
inputs into a hidden layer like I'm
21:30
doing here which is just defined as a
21:31
single dense layer and then taking those
21:35
hidden outputs feeding that into another
21:38
dense layer to produce your outputs the
21:42
final model is to find it end to end
21:44
with that single line at the end model
21:46
of inputs and outputs and that just
21:47
essentially connects the graph and to
21:49
end so now let's keep building on this
21:53
idea now we want to build a deep neural
21:55
network what is the deep neural network
21:57
well it's just one where we keep
21:58
stacking these hidden layers back to
22:00
back to back to back to create
22:01
increasingly deeper and deeper models
22:04
one where the output is computed by
22:07
going deeper into the network and
22:08
computing these weighted sums over and
22:11
over and over again with these
22:12
activation functions repeatedly applied
22:16
so this is awesome now we have an idea
22:17
on how to actually build a neural
22:19
network from scratch going all the way
22:21
from a single perceptron and we know how
22:24
to compose them to create very complex
22:25
deep neural networks as well let's take
22:28
a look at how we can apply this to a
22:30
very real problem that I know a lot of
22:33
you probably care about so I was
22:34
thinking of a problem potential that
22:37
some of you might care about it took me
22:39
a while but I think this might be one so
22:41
at MIT we care a lot about passing our
22:44
classes so I think a very good example
22:46
is let's train a neural network to
22:48
determine if you're gonna pass your
22:49
class so to do this let's start with a
22:53
simple two input feature model one
22:55
feature is the number of lectures that
22:57
you attend the other feature is the
22:59
number of hours that you spend on the
23:01
final project again since we have two
23:05
inputs we can plot this data on a
23:06
feature map like we did before green
23:09
points here represent previous students
23:11
from previous years that pass the class
23:13
red points represent students that
23:15
failed the class now if you want to find
23:17
out if you're gonna pass or fail to
23:19
class you can also apply yourself on
23:20
this map you spent you came to four
23:23
lectures spend five hours on your final
23:25
project and you want to know if you're
23:27
going to pass or fail and you want to
23:30
actually build a neural networks that's
23:31
going to learn this look at the old the
23:34
the previous people that took the scores
23:36
and determine if you all pass or fail as
23:38
well so let's do it
23:41
we have two inputs one is four one is
23:44
five these are fed into a single layered
23:46
neural network with three hidden units
23:48
and we see that the final output
23:50
probability that you will pass this
23:52
class is 0.1 or 10% not very good that's
23:58
actually really bad news can anyone
24:00
guess why this person who actually was
24:03
in the part of the feature space it
24:05
looked like they were actually in a good
24:07
part of this feature space looked like
24:08
they were gonna pass the class why did
24:10
this neural network give me such a bad
24:11
prediction here yeah exactly so the
24:16
network was not trained essentially this
24:19
network is like a baby that was just
24:20
born it has no idea of what lectures are
24:22
it doesn't know where final labs are it
24:24
doesn't know anything about this world
24:25
it's these are just numbers to it it's
24:28
been randomly initialized it has
24:29
no idea about the problem so we have to
24:31
actually train it we have to teach it
24:32
how to get the right answer so the first
24:36
thing that we have to do is tell the
24:37
network when it makes a mistake so that
24:39
we can correct it in the future now how
24:41
do we do this in neural networks the
24:43
loss of a network is actually what
24:45
defines when the network makes the wrong
24:47
prediction it takes the input and the
24:50
predicted output sorry it takes as input
24:53
the predicted output and the ground
24:55
truth actual output if your predicted
24:58
output and your ground truth output are
24:59
very close to each other then that
25:01
essentially means that your loss is
25:03
going to be very low you didn't make a
25:04
mistake but if your ground truth output
25:06
is very far away from your predicted
25:08
output that means that you should have a
25:10
very high loss you just have a lot of
25:12
error and your network should correct
25:13
that so let's assume that we have data
25:18
not just from one student now but we
25:21
have data from many many different
25:22
students passing and failing the class
25:25
we now care about how this model does
25:28
not just on that one student but across
25:30
the entire population of students and we
25:33
call this the empirical loss and that's
25:35
just the mean of all of the losses for
25:37
the individual students we can do it by
25:39
literally just computing the mean sorry
25:42
just computing the loss for each of
25:43
these students and taking their mean
25:45
when training a network what we really
25:48
want to do is not minimize the loss for
25:50
any particular student but we want to
25:51
minimize the loss across the entire
25:53
training set so if we go back to our
25:58
problem on path predicting if you'll
26:01
pass or fail to class this is a problem
26:03
of binary classification your output is
26:05
0 or 1 we already learned that when
26:08
outputs are 0 or 1 you're probably going
26:09
to want to use a soft max output for
26:13
those of you who aren't familiar with
26:15
cross entropy this was an idea
26:17
introduced actually at MIT and a
26:19
master's thesis here over 50 years ago
26:21
it's widely used in different areas like
26:22
thermodynamics and we use it here in
26:24
machine learning as well it's used all
26:26
over information theory and what this is
26:29
doing here is essentially computing the
26:31
loss between this zero one output and
26:34
the true output that the student either
26:37
passed or failed to class let's suppose
26:40
instead of computing a zero one output
26:42
now we want to
26:43
compute the actual grade that you will
26:45
get on the class so now it's not 0-1 but
26:47
it's actually a grade it could be any
26:49
number actually right now we want to use
26:52
a different loss because the output of
26:54
our net of our neural network is
26:56
different and defining losses is
26:58
actually kind of one of the arts in deep
27:00
learning so you have to define the
27:01
questions that you're asking so you can
27:03
define the loss that you need to
27:05
optimize over so here in this example
27:08
since we're not optimizing over zero one
27:10
loss we're optimizing over any real
27:12
number we're gonna use a mean squared
27:14
error loss and that's just computing the
27:17
squared error so you take the difference
27:19
between what you expect the output to be
27:21
and what you're actually output was you
27:23
take that difference you square it and
27:25
you compute the mean over your entire
27:27
population okay great
27:30
so now let's put some of this
27:32
information together we've learned how
27:33
to build neural networks we've learned
27:35
how to quantify their loss now we can
27:38
learn how to actually use that loss to
27:40
iteratively update and train the neural
27:43
network over time given some data and
27:46
essentially what this amounts to what
27:49
this boils down to is that we want to
27:51
find the weights of the neural network W
27:54
that minimize this empirical loss so
27:58
remember again the empirical loss is the
28:01
loss over the entire training set it's
28:03
the mean loss of all of the popular of
28:04
all of the individuals in the training
28:06
set and we want to minimize that loss
28:08
and that essentially means we want to
28:10
find the weights the parameterization of
28:13
the network that results in the minimum
28:16
loss remember again that W here is just
28:20
a collection it's just a set of all of
28:22
the weights in the network so before I
28:24
define W as W 0 W 1 which is the weights
28:27
for the first layer second layer third
28:29
layer etc and you keep stacking all of
28:33
these weights together you combine them
28:34
and you want to compute this
28:36
optimization problem over all of these
28:38
weights so again remember our loss
28:43
function what does our loss function
28:45
look like it's just a simple function
28:47
that takes as inputs our weights and if
28:51
we have two weights we can actually
28:53
visualize it again we can see on the
28:55
x-axis
28:57
one way so this is one scaler that we
28:59
can change and another way on the y axis
29:01
and on the z axis this is our actual
29:04
loss if we want to find the lowest point
29:08
in this landscape that corresponds to
29:10
the minimum loss and we want to find
29:12
that point so that we can find the
29:14
corresponding weights that were set to
29:16
achieve that minimum loss so how do we
29:20
do it we use this technique called loss
29:23
optimization through gradient descent we
29:26
start by picking an initial point on
29:29
this landscape an initial w0 w1 so
29:32
here's this point this black cross we
29:35
start at this point we compute the
29:38
gradient at this local point and in this
29:41
landscape we can see that the gradient
29:43
tells us the direction of maximal ascent
29:47
now that we know the direction of the
29:49
maximal ascent we can reverse that
29:50
gradient and actually take a small step
29:53
in the opposite direction that moves us
29:56
closer towards the lowest point because
29:58
we're taking a greedy approach to move
30:00
in the opposite direction of the
30:01
gradient we can iteratively repeat this
30:04
process over and over and over again we
30:07
computing the gradient at each time and
30:08
keep moving moving closer towards that
30:11
lowest minimum we can summarize this
30:15
algorithm known as gradient descent in
30:17
pseudocode by this the pseudocode on the
30:20
left-hand side we start by initializing
30:23
our weights randomly computing this
30:25
gradient DJ DW then updating our weights
30:30
in the opposite direction of that
30:31
gradient we used this small amount ADA
30:37
which you can see here and this is
30:40
essentially what we call the learning
30:42
rate this is determining how much of a
30:44
step we take and how much we trust that
30:46
with that gradient update that we
30:48
computed we'll talk more about this
30:50
later but for now let's take a look at
30:53
this term here this gradient DJ DW is
30:56
actually explaining how the lost changes
30:59
with respect to each of the weights but
31:03
I never actually told you how to compute
31:05
this term this is actually a crucial
31:07
part of deep learning and neural
31:09
networks in general
31:10
computing this term is essentially all
31:12
that matters when you try and optimize
31:15
your network is the most computational
31:17
part of training as well and it's known
31:19
as back propagation we'll start with a
31:22
very simple network with only one hidden
31:24
input sorry with one input one hidden
31:26
layer one handed and unit and one output
31:30
computing the gradient of our loss with
31:33
respect to W to corresponds to telling
31:37
us how much a small change in our and W
31:40
two affects our output or loss so if we
31:45
write this as a derivative we can start
31:48
by computing this by simply expanding
31:50
this derivative into a chain by using
31:52
the chain rule backwards from the loss
31:55
through the output and that looks like
31:58
this so DJ DW 2 becomes DJ dy dy DW 2 ok
32:06
and that's just a simple application of
32:07
the chain rule now let's suppose instead
32:11
of computing DJ DW 2 we want to compute
32:13
DJ DW 1 so I've changed the W 1 the W 2
32:17
to a W 1 on the left hand side and now
32:19
we want to compute this well we can
32:21
simply apply the chain rule again we can
32:24
take that middle term now expand it out
32:27
again using the same chain rule and back
32:29
propagate those gradients even further
32:30
back in in the network and essentially
32:35
we keep repeating this for every weight
32:37
in the network using the gradients for
32:39
later layers to back propagate those
32:41
errors back into the original input we
32:43
do this for all of the weights and and
32:45
that gives us our gradient for each
32:46
weight
32:58
yeah you're completely right so the
33:01
question is how do you ensure that this
33:02
gives you a global minimum instead of a
33:04
local minimum right so you don't we have
33:08
no guarantees on that this is not a
33:10
global minimum the whole training of
33:14
stochastic gradient sent is a greedy
33:15
optimization algorithm so you're only
33:17
taking this greedy approach and
33:18
optimizing only a local minimum there
33:21
are different ways extensions of
33:23
stochastic gradient descent that don't
33:25
take a greedy approach they take an
33:27
adaptive approach they look around a
33:29
little bit these are typically more
33:30
expensive to compute stochastic gradient
33:33
side is extremely cheap to compute in
33:35
practice and that's one of the reasons
33:37
it's used the second reason is that in
33:39
practice local minimum tend to be
33:42
sufficient so that's the back
33:47
propagation algorithm in theory it
33:51
sounds very simple it's just an
33:53
application of the chain rule but now
33:55
let's touch on some insights on training
33:58
these neural networks in practice that
33:59
makes it incredibly complex and this
34:01
gets back to that that previous point
34:02
that previous question that was raised
34:04
in practice training neural networks is
34:07
incredibly difficult this is a
34:08
visualization of the lost landscape of a
34:11
neural network in practice this is a
34:14
paper from about a year ago and the
34:16
authors visualize what a deep neural
34:17
network lost landscape really looks like
34:19
you can see many many many local minimum
34:22
here lot minimizing this loss and
34:24
finally the optimal true minimum is
34:26
extremely difficult
34:29
now recall the update equation that we
34:32
fought defined for a gradient descent
34:34
previously we take our weights and we
34:36
subtract we move towards the negative
34:40
gradient and we update our weights in
34:41
that direction I didn't talk too much
34:46
about this parameter heydo this is what
34:48
we called the learning rate I briefly
34:50
touched on it and this is essentially
34:52
determining how large of a step we take
34:54
at each iteration in practice setting
34:57
the learning rate can be extremely
34:59
difficult and actually very important
35:01
for making sure that you avoid local
35:03
minima again so if we set the learning
35:06
rate to slow then the model may get
35:08
stuck in local minimum like
35:10
this it could also converge very slowly
35:12
even in the case that it gets to a
35:13
global minimum if we set the learning
35:16
rate too large the gradients essentially
35:19
explodes and we diverge from the loss
35:21
itself and it's also been setting the
35:24
learning rate to the correct amount can
35:26
be extremely tedious in practice such
35:28
that we overshoot some of the local
35:30
minima get ourselves into a reasonable
35:32
local global minima and then converge in
35:35
within that global minima how can we do
35:40
this in a clever way so one option is
35:43
that we can try a lot of different
35:45
possible learning rates see what works
35:47
best in practice and in practice this is
35:51
actually a very common technique so a
35:53
lot of people just try a lot of learning
35:55
rates and see what works best let's see
35:57
if we can do something a bit smarter
35:59
than that as well how about we design an
36:01
adaptive algorithm that learnt that you
36:05
that adapts its learning rate according
36:07
to the lost landscape so this can take
36:10
into account the gradient at other
36:13
locations and loss it can take into
36:16
account how fast we're learning how how
36:18
large the gradient is at that location
36:20
or many other options but now since our
36:23
learning rate is not fixed for all of
36:25
the iterations of gradient descent we
36:28
have a bit more flexibility now in
36:29
learning in fact this has been widely
36:33
studied as well there are many many
36:34
different options for optimization
36:36
schemes that are present in tensorflow
36:38
and here are examples of some of them
36:41
during your labs I encourage you to try
36:43
out different of these different ones of
36:45
these optimizers and see how they're
36:47
different which works best which doesn't
36:49
work so well for your particular problem
36:51
and they're all adaptive in nature so
36:57
now I want to continue talking about
36:58
tips for training these networks in
37:01
practice and focus on the very powerful
37:03
idea of batching gradient descent and
37:06
batching your data in general so to do
37:10
this let's revisit this idea of gradient
37:12
descent very quickly so the gradient is
37:16
actually very computational to compute
37:18
this back propagation algorithm if you
37:20
want to compute it for all of the data
37:21
samples in your training data set which
37:23
may be
37:23
massive in modern data sets it's
37:26
essentially amounting to a summation
37:28
over all of these data points in most
37:32
real life problems this is extremely
37:34
computational and not feasible to
37:35
compute on every iteration so instead
37:38
people have come up with this idea of
37:41
stochastic gradient descent and that
37:42
involves picking a single point in your
37:44
data set computing the gradient with
37:47
respect to that point and then using
37:49
that to update your grade to update your
37:51
your weights so this is great because
37:55
now computing a gradient of a single
37:57
point is much easier than computing the
37:58
gradient over many points but at the
38:01
same time since we're only looking at
38:03
one point this can be extremely noisy
38:05
sure we take a different point each time
38:07
but still when we move and we take a
38:10
step in that direction of that point we
38:12
may be going in in a step that's not
38:14
necessarily representative of the entire
38:16
data set so is there a middle ground
38:21
such that we don't have to have a
38:23
stochastic a stochastic gradient but we
38:27
can still be kind of computationally
38:29
efficient in the sense so instead of
38:32
computing a noisy gradient of a single
38:34
point let's get a better estimate by
38:36
batching our data into mini batches of B
38:40
data points capital B data points so now
38:43
this gives us an estimate of the true
38:45
gradient by just averaging the gradient
38:47
from each of these points this is great
38:50
because now it's much easier to compute
38:53
than full gradient descent it's a lot
38:56
less points typically B is on the order
38:58
of less than 100 or approximately in
39:01
that range and it's a lot more accurate
39:04
than stochastic gradient descent because
39:06
you're considering a larger population
39:08
as well this increase in gradient
39:12
accuracy estimation actually allows us
39:14
to converge much quicker as well because
39:16
it means that we can increase our
39:17
learning rate and trust our gradient
39:18
more with each step which ultimately
39:22
means that we can train faster this
39:25
allows for massively parallel lyza
39:27
become potations because we can split up
39:28
batches across the GPU send batches all
39:32
over the GPU compute their gradients
39:34
simultaneously and then aggregate them
39:35
back
39:36
to even speed up even further now the
39:40
last topic I want to address before
39:42
ending is this idea of overfitting this
39:45
is one of the most fundamental problems
39:49
in machine learning as a whole not just
39:51
deep learning and at its core it
39:55
involves understanding the complexity of
39:59
your model so you want to build a model
40:00
that performs well and generalized as
40:02
well not just to your training set but
40:04
to your test set as well assume that you
40:09
want to build a model that describes
40:10
these points you can go on the left-hand
40:13
side which is just a line fitting a line
40:15
through these points this is under
40:16
fitting the complexity of your model is
40:19
not large enough to really learn the
40:21
full complexity of the data or you can
40:24
go on the right-hand side which is
40:25
overfitting where you're essentially
40:27
building a very complex model to
40:30
essentially memorize the data and this
40:32
is not useful either because when you
40:33
show a new data it's not going to sense
40:36
it's not going to perfectly match on the
40:38
training data and it means that you're
40:39
going to have high generalization error
40:42
ideally we want to end up with a model
40:44
in the middle that is not too complex to
40:46
memorize all of our training data but
40:48
still able to generalize and perform
40:51
well even we have when we have brand new
40:53
training and testing inputs so to
40:57
address this problem let's talk about
40:58
regularization for deep neural networks
41:01
deep neural regularization is a
41:03
technique that you can introduce to your
41:05
networks that will discourage complex
41:08
models from being learned and as before
41:12
we've seen that it's crucial for our
41:14
models to be able to generalize to data
41:18
beyond our training set but also to
41:20
generate generalize to data in our
41:23
testing set as well the most popular
41:26
regularization technique in deep
41:28
learning is a very simple idea called
41:30
dropout let's revisit this and a picture
41:32
of a deep neural network again and drop
41:35
out during training we randomly set some
41:38
of our activations of the hidden neurons
41:40
to 0 with some probability that's why we
41:43
call it dropping out because we're
41:44
essentially killing off those neurons so
41:46
let's do that so we kill off these
41:48
random sample of neurons
41:49
and now we've created a different
41:51
pathway through the network let's say
41:53
that you dropped 50 percent of the
41:55
neurons this means that those
41:57
activations are set to zero and the
41:59
network is not going to rely too heavily
42:01
on any particular path through the
42:03
network but it's instead going to find a
42:05
whole ensemble of different paths
42:07
because it doesn't know which path is
42:08
going to be dropped out at any given
42:10
time we repeat this process on every
42:13
training iteration now dropping out a
42:15
new set of 50 50 % of the neurons and
42:20
the result of this is essentially a
42:22
model that like I said creates an
42:25
ensemble of multiple models through the
42:27
paths of the network and is able to
42:29
generalize better to unseen test data so
42:35
the second technique for a
42:36
regularization is this notion that we'll
42:41
talk about which is early stopping and
42:44
the idea here is also extremely simple
42:47
let's train our neural network like
42:48
before no dropout but let's just stop
42:51
training before we have a chance to
42:52
overfit so we start training and the
42:56
definition of overfitting is just when
42:57
our model starts to perform worse on the
42:59
test set then on the training set so we
43:04
can start off and we can plot how our
43:05
loss is going for both the training and
43:07
test set we can see that both are
43:09
decreasing so we keep training now we
43:11
can see that the training the validation
43:13
both losses are kind of starting to
43:14
plateau here we can keep going the
43:17
training loss is always going to decay
43:19
it's always going to keep decreasing
43:20
because especially if you have a network
43:22
that is having such a large capacity to
43:26
essentially memorize your data you can
43:28
always perfectly get a training accuracy
43:30
of 0 that's not always the case but in a
43:33
lot of times with deep neural networks
43:34
since they're so expressive and have so
43:37
many weights they're able to actually
43:39
memorize the data if you let them train
43:41
for too long if we keep training like
43:44
you see the training set continues to
43:46
decrease now the validation set starts
43:48
to increase and if we keep doing this
43:53
the trend continues the idea of early
43:56
stopping is essentially that we want to
43:57
focus on this point here and stop
44:00
training when we reach this point so we
44:03
can key
44:04
basically records of the model during
44:06
training and once we start to detect
44:09
overfitting we can just stop and take
44:11
that last model that was still occurring
44:14
before the overfitting happened right so
44:18
on the left hand side you can see the
44:20
under fitting you don't want to stop too
44:22
early you want to let the model get the
44:24
minimum validation set accuracy but also
44:27
you don't want to keep training such
44:29
that the validation accuracy starts the
44:31
increase on the other end as well so
44:34
I'll conclude this first lecture by
44:36
summarizing three key points that we
44:37
have covered so far first we learned
44:40
about the fundamental building blocks of
44:42
deep learning which is just a single
44:43
neuron or called the perceptron we
44:46
learned about back propagation how to
44:48
stack these neurons into complex deep
44:51
neural networks how to back propagate
44:53
and errors through them and learn
44:55
complex loss functions and finally we
45:00
discussed some of the practical details
45:02
and tricks to training neural networks
45:04
that are really crucial today if you
45:07
want to work in this field such as
45:10
batching regularization and and others
45:13
so now I'll take any questions or if
45:16
there are no questions and I'm gonna
45:19
hand the mic over to ovah who will talk
45:20
about sequence modeling thank you
45:25
[Applause]